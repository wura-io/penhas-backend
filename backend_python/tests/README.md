# PenhaS Python Backend - Testing Guide

## ğŸ§ª Testing Infrastructure

This directory contains the complete testing suite for the PenhaS Python backend.

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ conftest.py              # Pytest configuration and fixtures
â”œâ”€â”€ test_auth.py            # Authentication and security tests
â”œâ”€â”€ test_integration.py     # API integration tests
â”œâ”€â”€ test_e2e.py            # End-to-end user flow tests
â””â”€â”€ README.md              # This file
```

## ğŸš€ Running Tests

### Run all tests
```bash
poetry run pytest
```

### Run specific test file
```bash
poetry run pytest tests/test_auth.py
```

### Run with coverage
```bash
poetry run pytest --cov=app --cov-report=html
```

### Run with verbose output
```bash
poetry run pytest -v -s
```

### Run only fast tests (skip slow integration tests)
```bash
poetry run pytest -m "not slow"
```

## ğŸ“Š Test Categories

### Unit Tests (`test_auth.py`)
- âœ… Password hashing and verification
- âœ… JWT token creation and validation
- âœ… Legacy SHA256 password support
- âœ… Utility functions (PII removal, UUID validation)
- âœ… User model operations

**Coverage**: Core security and business logic

### Integration Tests (`test_integration.py`)
- âœ… API endpoint responses
- âœ… Authentication flows
- âœ… Database operations
- âœ… Error handling
- âœ… OpenAPI schema generation

**Coverage**: Full request/response cycles

### E2E Tests (`test_e2e.py`)
- âœ… Complete user registration flow
- âœ… Guardian management flow
- âœ… Timeline interaction flow
- âœ… Password reset flow
- âœ… Chat functionality
- âœ… Audio management
- âœ… Admin workflows

**Coverage**: Complete user journeys

## ğŸ”§ Configuration

### Test Database
Tests use a separate test database: `penhas_test`

Configure in `.env`:
```bash
POSTGRESQL_DBNAME=penhas
# Test database will be penhas_test
```

### Fixtures
Common fixtures are defined in `conftest.py`:
- `db`: AsyncSession for database operations
- `event_loop`: Async event loop
- `engine`: Test database engine
- `sample_user_data`: Sample user data

## ğŸ“ Writing New Tests

### Unit Test Example
```python
def test_something():
    """Test description"""
    result = function_to_test()
    assert result == expected_value
```

### Async Test Example
```python
@pytest.mark.asyncio
async def test_async_function(db: AsyncSession):
    """Test async function"""
    result = await async_function(db)
    assert result is not None
```

### Integration Test Example
```python
@pytest.mark.asyncio
async def test_endpoint():
    """Test API endpoint"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.get("/endpoint")
        assert response.status_code == 200
```

## ğŸ¯ Coverage Goals

Target coverage by module:
- **Core modules**: 80%+
- **Helpers**: 70%+
- **Endpoints**: 60%+
- **Overall**: 70%+

## ğŸ” Test Organization Best Practices

1. **One test file per module** or logical grouping
2. **Use descriptive test names** that explain what's being tested
3. **Arrange-Act-Assert** pattern:
   ```python
   def test_something():
       # Arrange - Set up test data
       data = setup_data()
       
       # Act - Execute the code
       result = function_under_test(data)
       
       # Assert - Check results
       assert result == expected
   ```
4. **Use fixtures** for common setup
5. **Mock external services** (AWS, Google Maps, etc.)

## ğŸ› Debugging Tests

### Run with print statements
```bash
pytest -v -s
```

### Run specific test
```bash
pytest tests/test_auth.py::TestPasswordHashing::test_password_hash
```

### Drop into debugger on failure
```bash
pytest --pdb
```

## ğŸ“š Additional Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [FastAPI Testing](https://fastapi.tiangolo.com/tutorial/testing/)
- [SQLAlchemy Async Testing](https://docs.sqlalchemy.org/en/14/orm/extensions/asyncio.html)

## âœ… Test Checklist

Before merging code, ensure:
- [ ] All tests pass
- [ ] New features have tests
- [ ] Coverage hasn't decreased
- [ ] Integration tests pass
- [ ] E2E flows work
- [ ] No warnings or errors

## ğŸ‰ Testing Status

**Current Status**: Infrastructure ready, example tests implemented

**Next Steps**:
1. Run full test suite: `pytest`
2. Generate coverage report: `pytest --cov=app --cov-report=html`
3. Review coverage and add tests for uncovered code
4. Set up CI/CD to run tests automatically

---

*The testing infrastructure is production-ready and awaits comprehensive test implementation!*

